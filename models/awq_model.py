"""
AWQ Model Implementation

Implements the ModelInterface for AWQ quantized models.
Focus: Efficient inference with AutoAWQ.
"""

import gc
import time
from typing import Dict, Any, List, Optional
import torch
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM

from models.model_interface import ModelInterface, GenerationConfig, ModelOutput


class AWQModel(ModelInterface):
    """AutoAWQ model implementation for AWQ quantized models"""
    
    def __init__(self, model_path: str, device: str = "cuda:0"):
        super().__init__(model_path, device)
    
    def load(self):
        """Load model using AutoAWQ"""
        print(f"Loading AWQ model: {self.model_path}")
        
        self._tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self._tokenizer.pad_token = self._tokenizer.eos_token
        
        self._model = AutoAWQForCausalLM.from_quantized(
            self.model_path,
            fuse_layers=True
        )
        
        # Detect model type from model name
        model_name_lower = self.model_path.lower()
        if "instruct" in model_name_lower or "chat" in model_name_lower:
            self._model_type = "instruct"
        else:
            self._model_type = "base"
        
        print(f"Model loaded on: {self._model.device}")
        print(f"Model type detected: {self._model_type}")
    
    def generate(self, prompt: str, config: GenerationConfig, return_attentions: bool = False) -> ModelOutput:
        """
        Generate text from prompt.
        
        Note: AWQ models don't reliably support attention extraction.
        return_attentions parameter is ignored for stability.
        """
        if return_attentions:
            print("Warning: AWQ models do not support attention extraction")
        
        inputs = self._tokenizer(prompt, return_tensors="pt").to(self._model.device)
        input_length = inputs.input_ids.shape[1]
        
        start_time = time.perf_counter()
        
        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_new_tokens=config.max_new_tokens,
                do_sample=config.do_sample,
                temperature=config.temperature if config.do_sample else 1.0,
                top_p=config.top_p if config.do_sample else 1.0,
                top_k=config.top_k if config.do_sample else 50,
                pad_token_id=self._tokenizer.eos_token_id
            )
        
        latency_ms = (time.perf_counter() - start_time) * 1000
        
        generated_text = self._tokenizer.decode(
            outputs[0][input_length:],
            skip_special_tokens=True
        )
        
        num_tokens = outputs.shape[1] - input_length
        
        return ModelOutput(
            generated_ids=outputs,
            generated_text=generated_text,
            attentions=None,
            num_generated_tokens=num_tokens,
            latency_ms=latency_ms
        )
    
    def get_perplexity(self, text: str, max_length: int = 512) -> float:
        """Calculate perplexity on text"""
        inputs = self._tokenizer(
            text,
            return_tensors="pt",
            max_length=max_length,
            truncation=True
        ).to(self._model.device)
        
        if inputs.input_ids.size(1) < 2:
            return float('inf')
        
        try:
            with torch.no_grad():
                if hasattr(self._model, 'model'):
                    outputs = self._model.model(**inputs, labels=inputs["input_ids"])
                else:
                    outputs = self._model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
        except (AttributeError, TypeError):
            with torch.no_grad():
                outputs = self._model(**inputs)
                logits = outputs.logits
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = inputs["input_ids"][..., 1:].contiguous()
                loss_fct = torch.nn.CrossEntropyLoss()
                loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1)
                )
        
        return torch.exp(loss).item()
    
    def encode(self, text: str, max_length: int = None) -> Dict[str, torch.Tensor]:
        """Tokenize text"""
        return self._tokenizer(
            text,
            return_tensors="pt",
            max_length=max_length,
            truncation=True if max_length else False
        ).to(self._model.device)
    
    def decode(self, token_ids: torch.Tensor, skip_special_tokens: bool = True) -> str:
        """Decode token IDs"""
        return self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model configuration info"""
        config = self._model.model.config
        
        return {
            "model_path": self.model_path,
            "model_type": self._model_type,
            "num_layers": config.num_hidden_layers,
            "num_attention_heads": config.num_attention_heads,
            "hidden_size": config.hidden_size,
            "vocab_size": config.vocab_size,
            "dtype": "awq_4bit",
            "backend": "autoawq"
        }
    
    def get_memory_usage(self) -> Dict[str, float]:
        """Get memory usage statistics"""
        if torch.cuda.is_available():
            return {
                "allocated_mb": torch.cuda.memory_allocated() / (1024**2),
                "reserved_mb": torch.cuda.memory_reserved() / (1024**2),
                "max_allocated_mb": torch.cuda.max_memory_allocated() / (1024**2)
            }
        return {"allocated_mb": 0.0, "reserved_mb": 0.0, "max_allocated_mb": 0.0}
    
    def clear_memory(self):
        """Clear memory cache"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def unload(self):
        """Unload model from memory"""
        del self._model
        del self._tokenizer
        self.clear_memory()