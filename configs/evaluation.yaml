experiment:
  name: "mistral-7b-quantization-rag-evaluation"
  description: "Quantization effects on Mistral-7B for RAG with long-context"
  hardware: "Kaggle T4 GPU (16GB VRAM)"
  base_model: "mistralai/Mistral-7B-Instruct-v0.2"
  seed: 42

models:
  fp16_baseline:
    path: "mistralai/Mistral-7B-Instruct-v0.2"
    type: "hf"
    description: "FP16 baseline"

  awq_g128:
    path: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
    type: "awq"
    description: "AWQ 4-bit group_size=128"

  awq_g64:
    path: "solidrust/Mistral-7B-Instruct-v0.2-AWQ"
    type: "awq"
    description: "AWQ 4-bit group_size=64"

  gptq:
    path: "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
    type: "gptq"
    description: "GPTQ 4-bit"

lm_eval:
  enabled: true
  baseline_reasoning:
    tasks:
      - hellaswag
      - winogrande
      - piqa
      - arc_easy
      - arc_challenge
    num_fewshot: 0
    batch_size: 1

  core_rag:
    tasks:
      - squad_v2
      - triviaqa
    num_fewshot: 0
    batch_size: 1

  rag_reasoning:
    tasks:
      - drop
      - race
    num_fewshot: 3
    batch_size: 1

  verification:
    tasks:
      - boolq
    num_fewshot: 0
    batch_size: 1

performance:
  enabled: true
  num_warmup: 3
  num_runs: 10
  max_new_tokens: 128

perplexity:
  enabled: true
  dataset: "HuggingFaceH4/ultrachat_200k"
  config: null
  split: "train_sft"
  max_samples: 50
  max_length: 512
  min_text_length: 100

context_length:
  enabled: true
  context_lengths: [512, 1024, 2048, 4096]
  samples_per_length: 5
  test_positions: ["start", "middle", "end"]

needle_in_haystack:
  enabled: true
  context_lengths: [2048, 4096]
  position_percentiles: [10, 50, 90]
  num_needles: 3

rag_evaluation:
  enabled: true
  dataset: "squad_v2"
  max_samples: 100

  pipeline_config:
    document_processing:
      remove_headers: true
      remove_citations: true
      extract_sections: false

    chunking:
      strategy: "semantic"
      chunk_size: 512
      chunk_overlap: 50
      min_chunk_size: 100

    embedding:
      model_name: "sentence-transformers/all-MiniLM-L6-v2"
      batch_size: 32
      normalize: true
      device: "cuda"

    vector_store:
      collection_name: "rag_eval"
      persist_directory: null

    retrieval:
      top_k: 3
      similarity_threshold: 0.0
      rerank: false
      diversity_penalty: 0.0

    generation:
      max_new_tokens: 64
      temperature: 0.3
      top_p: 0.9
      do_sample: true
      repetition_penalty: 1.15
      use_chat_template: true

output:
  dir: "results"
  save_detailed: true
  log_level: "INFO"
  formats: ["json", "csv"]

constraints:
  max_memory_gb: 16
  device: "cuda:0"
  max_context_length: 4096
